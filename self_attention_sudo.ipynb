{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLhTE8tCzJgb/Xn+vyXTOs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarvinSaravi/Sharif_Machine_Learning/blob/srv/self_attention_sudo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q51TkLSJwOni",
        "outputId": "38a78c95-54e0-4f8f-8442-34693f7d97b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output shape: (5, 64)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(x, axis=-1):\n",
        "    x_exp = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
        "    return x_exp / np.sum(x_exp, axis=axis, keepdims=True)\n",
        "\n",
        "\n",
        "N = 5   # Sequence length\n",
        "d_model = 64  # Model embedding dimension\n",
        "d_k = d_v = d_model # for simplicity, use single-head and no dimension reduction\n",
        "\n",
        "\n",
        "X = np.random.randn(N, d_model)\n",
        "\n",
        "W_Q = np.random.randn(d_model, d_k)\n",
        "W_K = np.random.randn(d_model, d_k)\n",
        "W_V = np.random.randn(d_model, d_v)\n",
        "\n",
        "# Attention Mechanism\n",
        "Q = X @ W_Q   #[N, d_k]\n",
        "K = X @ W_K   #[N, d_k]\n",
        "V = X @ W_V   #[N, d_v]\n",
        "\n",
        "scores = Q @ K.T  #[N, N]\n",
        "\n",
        "scores = scores / np.sqrt(d_k)\n",
        "\n",
        "alpha = softmax(scores, axis=-1)  #[N, N]\n",
        "\n",
        "O = alpha @ V   #[N, d_v]\n",
        "\n",
        "print(\"output shape:\", O.shape)\n",
        "\n"
      ]
    }
  ]
}